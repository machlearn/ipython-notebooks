{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests belong to the class of ensemble methods. The goal of ensemble methods is to combine the predictions of several base estimators built with a give learning algorithm in order to improve generalizability/ robustness over a single estimator. \n",
    "\n",
    "There are two families of ensemble methods:\n",
    "\n",
    "- Average methods: build several estimators independently and then to average their predictions. Examples: Bagging methods, Forest of randomized trees.\n",
    "\n",
    "- Boosting methods: base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. Examples: AdaBoost, Gradient Tree Boosting,...\n",
    "\n",
    "## Bagging meta-estimator\n",
    "\n",
    "- Build several instances of a blackbox estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. \n",
    "\n",
    "- In `scikit-learn`, bagging methods are offered as a unified `BaggingClassifier` meta-estimator, taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(), max_samples = 0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forests of randomized trees\n",
    "The `sklearn.ensemble` module includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method. These two has perturb-and-combine style: a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0,0], [1,1]]\n",
    "Y = [0, 1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Each tree is the ensemble built from a sample drawn with replacement from the training set. \n",
    "\n",
    "The scikit-learn implementation combines classifiers by averaging their probablistic prediction, instead of letting each classifier vote for a single class.\n",
    "\n",
    "### Extremely Randomized Trees\n",
    "\n",
    "Randomness goes one step further in the way splits are computed. \n",
    "\n",
    "As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random at each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. \n",
    "\n",
    "\n",
    "This allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias. \n",
    "\n",
    "### Parameters\n",
    "\n",
    "`n_estimators` and `max_features`.\n",
    "\n",
    "`n_estimators` is the number of tres in the forest. The larger the better, but also the longer it will take to compute. \n",
    "\n",
    "`max_features` is the size of random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater in increase in bias. \n",
    "\n",
    "Empirical good default values are `max_features=n_features` for regression problems, and `max_features=sqrt(n_features)` for classification tasks. \n",
    "\n",
    "Good results are often achieved when `max_depth=None` in combination with `min_samples_split=1`. \n",
    "\n",
    "The best parameter values should always be cross-validated. \n",
    "\n",
    "In random forests, bootstrap samples are used by default (`bootstrap=True`) while the default strategy for extra-trees is to use the whole dataset (`bootstrap=False`). \n",
    "\n",
    "### Parallelization \n",
    "\n",
    "`n_jobs`\n",
    "\n",
    "If `n_jobs=k` then computations are partitioned into k jobs, and run on k cores of the machine.\n",
    "\n",
    "IF `n_jobs=-1` then all cores available on the machine are used. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo \n",
    "- Read about decision trees\n",
    "- What does \"bootstrap samples\" mean? \n",
    "- See some examples in `scikit-learn` about the performance of random forests. \n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#example-ensemble-plot-forest-iris-py\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html#example-ensemble-plot-forest-importances-faces-py\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html#example-plot-multioutput-face-completion-py\n",
    "\n",
    "\n",
    "## References\n",
    "http://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
